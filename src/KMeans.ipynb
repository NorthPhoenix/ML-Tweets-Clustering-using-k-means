{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Nikita Istomin - CS4375 - Assignment 3 - Tweets Clustering using k-means\n",
    "    Date: April 23rd, 2023\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "\n",
    "plt.style.use(['ggplot'])\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_dataset(url: str) -> None:\n",
    "    \"\"\"Fetch a txt dataset from a public url and return it as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        url (str): public url to a txt dataset\n",
    "\n",
    "    Returns:\n",
    "        dataset (np.array): dataset as a numpy array\n",
    "    \"\"\"\n",
    "    with urlopen(url=url) as data:\n",
    "        dataset = np.array(data.read().decode('utf-8').splitlines())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_data: np.ndarray) -> None:\n",
    "        \"\"\"Preprocess the dataset\n",
    "\n",
    "        Args:\n",
    "            raw_data (np.ndarray): raw dataset as a numpy array\n",
    "\n",
    "        Returns:\n",
    "            data (np.ndarray): preprocessed dataset as a numpy array\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Perform the following pre-processing steps:\n",
    "            1) Remove the tweet id and timestamp\n",
    "            2) Remove any word that starts with the symbol @ e.g. @AnnaMedaris\n",
    "            3) Remove any hashtag symbols e.g. convert #depression to depression\n",
    "            4) Remove any URL\n",
    "            5) Convert every word to lowercase\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for tweet in raw_data:\n",
    "            tweet = tweet.split('|')\n",
    "            if len(tweet) != 3:\n",
    "                continue # skip tweets that don't have 3 parts\n",
    "            tweet = tweet[2] # remove tweet id and timestamp\n",
    "            tweet = tweet.split() # split tweet into words\n",
    "            tweet = [word.lower() for word in tweet if not word.startswith('@')] # remove words that start with @\n",
    "            tweet = [word[1:] if word.startswith('#') else word for word in tweet] # remove hashtag symbol\n",
    "            tweet = [word for word in tweet if not word.startswith('http')] # remove urls\n",
    "            # convert tweet to numpy array\n",
    "            tweet = np.array(tweet)\n",
    "            data.append(tweet)\n",
    "        \n",
    "        data = np.array(data, dtype=object)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    \"\"\"KMeans clustering algorithm\n",
    "\n",
    "    Attributes:\n",
    "        k (int): number of clusters\n",
    "        max_iter (int): maximum number of iterations\n",
    "        centroids (np.ndarray): centroid tweets of each cluster\n",
    "        clusters (np.ndarray): indices of tweets in each cluster\n",
    "        sse (np.ndarray): history of sum of squared errors (SSE)\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, max_iter: int = 10) -> None:\n",
    "        \"\"\"Initialize KMeans clustering algorithm\n",
    "\n",
    "        Args:\n",
    "            k (int): number of clusters\n",
    "            max_iter (int): maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.sse = np.array([])\n",
    "\n",
    "    def fit(self, data: np.ndarray) -> None:\n",
    "        \"\"\"Fit the data to the KMeans clustering algorithm\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): dataset as a numpy array\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        self._initialize_centroids(data)\n",
    "        finish = time.time()\n",
    "        print(f'Initialization time: {finish - start} seconds')\n",
    "        previous_centroids = np.copy(self.centroids)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            print(f'Iteration {i + 1}')\n",
    "\n",
    "            start = time.time()\n",
    "            self._assign_clusters(data)\n",
    "            finish = time.time()\n",
    "            print(f'Assignment time: {finish - start} seconds')\n",
    "\n",
    "            start = time.time()\n",
    "            self._update_centroids(data)\n",
    "            finish = time.time()\n",
    "            print(f'Update time: {finish - start} seconds')\n",
    "\n",
    "            self.sse = np.append(self.sse, self._SSE(data))\n",
    "            print(f'SSE: {self.sse[-1]}')\n",
    "            \n",
    "            # if the centroids don't change, stop the algorithm\n",
    "            if np.array_equal(self.centroids, previous_centroids):\n",
    "                break\n",
    "            previous_centroids = np.copy(self.centroids)\n",
    "\n",
    "    def visualize(self) -> None:\n",
    "        \"\"\"Visualize the SSE history\"\"\"\n",
    "        plt.plot(self.sse)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('SSE')\n",
    "        plt.title('SSE History')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _SSE(self, data: np.ndarray) -> float:\n",
    "        \"\"\"Compute the sum of squared errors (SSE)\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): dataset as a numpy array\n",
    "\n",
    "        Returns:\n",
    "            SSE (float): sum of squared errors\n",
    "        \"\"\"\n",
    "        SSE = 0\n",
    "        for i, cluster in enumerate(self.clusters):\n",
    "            SSE += np.sum(np.array([self._compute_distance(data[tweet], self.centroids[i])**2 for tweet in cluster]))\n",
    "        return SSE\n",
    "        \n",
    "\n",
    "    def _initialize_centroids(self, data: np.ndarray) -> None:\n",
    "        \"\"\"Initialize the centroids\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): dataset as a numpy array\n",
    "        \"\"\"\n",
    "        # randomly select k data points as initial centroids\n",
    "        self.centroids = data[np.random.choice(data.shape[0], self.k, replace=False)]\n",
    "\n",
    "    def _assign_clusters(self, data: np.ndarray) -> None:\n",
    "        \"\"\"Assign the data to the closest cluster\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): dataset as a numpy array\n",
    "        \"\"\"\n",
    "        self.clusters = np.zeros(self.k, dtype=np.ndarray)\n",
    "        for i, tweet in enumerate(data):\n",
    "            # compute the distance between the tweet and each centroid\n",
    "            distances = np.zeros(self.k)\n",
    "            for j, centroid in enumerate(self.centroids):\n",
    "                start = time.time()\n",
    "                distances[j] = self._compute_distance(tweet, centroid)\n",
    "                finish = time.time()\n",
    "                # print(f'Distance {j+1} computation time: {finish - start} seconds')\n",
    "            # assign the tweet to the closest cluster\n",
    "            start = time.time()\n",
    "            min_distance = np.argmin(distances)\n",
    "            self.clusters[min_distance] = np.append(self.clusters[min_distance], i)\n",
    "            finish = time.time()\n",
    "            # print(f'Cluster assignment time: {finish - start} seconds')\n",
    "\n",
    "\n",
    "    def _update_centroids(self, data: np.ndarray) -> None:\n",
    "        \"\"\"Update the centroids to the tweet with the lowest distance to other tweets in the cluster\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): dataset as a numpy array\n",
    "        \"\"\"\n",
    "        for i, cluster in enumerate(self.clusters):\n",
    "            # compute the distance between each tweet in the cluster and every other tweet in the cluster\n",
    "            start = time.time()\n",
    "            distances = np.array([[self._compute_distance(data[tweet1], data[tweet2]) for tweet2 in cluster] for tweet1 in cluster])\n",
    "            finish = time.time()\n",
    "            print(f'Distance {i+1} computation time: {finish - start} seconds')\n",
    "            # update the centroid to the tweet with the lowest mean of distances to other tweets in the cluster\n",
    "            self.centroids[i] = data[cluster[np.argmin(np.mean(distances, axis=1))]]\n",
    "        \n",
    "    def _compute_distance(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the Jaccard Distance between two points\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): first point\n",
    "            y (np.ndarray): second point\n",
    "\n",
    "        Returns:\n",
    "            distance (float): Jaccard Distance between two points\n",
    "        \"\"\"\n",
    "        # Jacard Distance = 1 - (intersection of two sets / union of two sets)\n",
    "\n",
    "        distance = 1 - (np.intersect1d(x, y).size / np.union1d(x, y).size)\n",
    "        return distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansGrid():\n",
    "    def __init__(self, k: list, max_iter: list) -> None:\n",
    "        self.params = {'k': k, 'max_iter': max_iter}\n",
    "        self.models = []\n",
    "        self.table = pd.DataFrame(columns=['k', 'SSE', 'Cluster Size'])\n",
    "    def run(self, data: np.ndarray) -> None:\n",
    "        for k in self.params['k']:\n",
    "            for max_iter in self.params['max_iter']:\n",
    "                print(f'k = {k}, max_iter = {max_iter}')\n",
    "                kmean = KMeans(k=k, max_iter=max_iter)\n",
    "                self.models.append(kmean)\n",
    "                kmean.fit(data)\n",
    "                kmean.visualize()\n",
    "                self.table = self.table.append(pd.DataFrame({'k': kmean.k, 'SSE': kmean.sse[-1], 'Cluster Size': [[cluster.size for cluster in kmean.clusters]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KMeans clustering algorithm\n",
    "params = {'k': [3, 5, 7, 10, 15], 'max_iter': [5]}\n",
    "kmeans = KMeansGrid(**params)\n",
    "# Fetch dataset\n",
    "print(\"Fetching dataset... \", end=\"\")\n",
    "dataset: np.ndarray = fetch_dataset(url='https://raw.githubusercontent.com/NorthPhoenix/ML-Tweets-Clustering-using-k-means/main/Datasets/usnewshealth.txt')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "print(\"Preprocessing dataset... \", end=\"\")\n",
    "dataset = preprocess(raw_data=dataset)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting dataset... \", end=\"\")\n",
    "kmeans.run(dataset)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k          SSE                                       Cluster Size\n",
      "0   3  1168.822648                                    [839, 418, 146]\n",
      "0   5  1140.615147                          [255, 521, 186, 156, 287]\n",
      "0   7  1137.947644                   [400, 472, 42, 199, 74, 99, 121]\n",
      "0  10  1128.161758       [239, 301, 244, 35, 91, 282, 20, 54, 78, 66]\n",
      "0  15  1100.883599  [145, 97, 82, 49, 144, 126, 300, 39, 18, 83, 1...\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "553999aee36a1f90cca16a986447380e01bd2761201c9a248cb76bac0699688d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
